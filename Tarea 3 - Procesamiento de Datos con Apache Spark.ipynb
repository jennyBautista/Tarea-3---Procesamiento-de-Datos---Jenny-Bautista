{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a1ddc0-bd3e-4107-a878-088a67107ef9",
   "metadata": {},
   "source": [
    "## Tarea 3 - Procesamiento de Datos con Apache Spark\n",
    "### Elaborado por: Jenny Bautista Vargas\n",
    "### Grupo: 85 - UNAD - Big Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f12308-c343-4c16-a864-0943b7982c7d",
   "metadata": {},
   "source": [
    "Este proyecto implementa una arquitectura Big Data usando Apache Spark y Apache Kafka, orientada al análisis en tiempo real de la Tasa Representativa del Mercado (TRM) publicada por la Superintendencia Financiera de Colombia.\n",
    "\n",
    "El flujo completo abarca desde la descarga del dataset original en formato CSV, su procesamiento en modo batch, hasta la simulación de llegada de datos en tiempo real mediante un productor Kafka y el consumo con Spark Structured Streaming.\n",
    "\n",
    "## Dataset Seleccionado: TRM - Tasa Representativa del Mercado: \n",
    "\n",
    "La Tasa Representativa del Mercado (TRM) refleja el valor del dólar estadounidense (USD) en pesos colombianos (COP), determinado diariamente por la Superintendencia Financiera: https://www.datos.gov.co/Econom-a-y-Finanzas/Tasa-de-Cambio-Representativa-del-Mercado-TRM/32sa-8pi3/about_data \n",
    "\n",
    "Campo\tDescripción\n",
    "VALOR\tValor de la tasa representativa en pesos colombianos (COP/USD).\n",
    "UNIDAD\tUnidad monetaria (generalmente “COP/USD”).\n",
    "VIGENCIADESDE\tFecha de inicio de vigencia de la tasa.\n",
    "VIGENCIAHASTA\tFecha final de vigencia (generalmente igual al día siguiente).\n",
    "\n",
    "## Procesamiento Batch en Apache Spark: \n",
    "Cargar el conjunto de datos, limpiarlo, transformarlo y realizar un análisis exploratorio (EDA) usando RDDs y DataFrames. El propósito del script tarea3.py es procesar en modo batch el conjunto de datos de la TRM, aplicando pasos de limpieza, transformación, análisis exploratorio (EDA) y finalmente almacenamiento de resultados en formato optimizado (Parquet) en HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6735b0ac-c9d9-43cb-85a7-aef162c2e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descargar los datos\n",
    "wget https://www.datos.gov.co/api/views/mcec-87by/rows.csv -O rows.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b5d6e6-92de-420d-b457-7a338a8d3d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iniciar park\n",
    "pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0bbc45-dd4a-418c-ae56-c468feda4a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar y procesar el dataset\n",
    "# tarea3.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "# Crear la sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Tarea3_ProcesamientoBatch\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Ruta donde subí el archivo a HDFS\n",
    "file_path = \"hdfs://localhost:9000/Tarea3/rows.csv\"\n",
    "\n",
    "# Leer archivo CSV desde HDFS como DataFrame\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(file_path)\n",
    "\n",
    "print(\"Dataset cargado correctamente desde HDFS\")\n",
    "df.printSchema()\n",
    "df.show(10, truncate=False)\n",
    "\n",
    "# Eliminar filas con valores nulos en columnas clave\n",
    "df_clean = df.dropna(subset=[\"VALOR\"])\n",
    "\n",
    "# Convertir a tipo double la columna VALOR si es string\n",
    "df_clean = df_clean.withColumn(\"VALOR\", F.col(\"VALOR\").cast(\"double\"))\n",
    "\n",
    "# Eliminar posibles duplicados\n",
    "df_clean = df_clean.dropDuplicates()\n",
    "\n",
    "# Crear una columna de año si existen columnas de fecha\n",
    "if \"VIGENCIADESDE\" in df_clean.columns:\n",
    "    df_clean = df_clean.withColumn(\"AÑO_INICIO\", F.year(F.to_date(\"VIGENCIADESD>\n",
    "\n",
    "print(\"Datos limpiados y transformados:\")\n",
    "df_clean.show(10, truncate=False)\n",
    "\n",
    "print(\"Estadísticas generales:\")\n",
    "df_clean.describe().show()\n",
    "\n",
    "# Valor promedio y máximo\n",
    "df_clean.agg(\n",
    "    F.avg(\"VALOR\").alias(\"Promedio_VALOR\"),\n",
    "    F.max(\"VALOR\").alias(\"Máximo_VALOR\"),\n",
    "    F.min(\"VALOR\").alias(\"Mínimo_VALOR\")\n",
    ").show()\n",
    "\n",
    "# Top 10 de valores más altos\n",
    "print(\"Top 10 valores más altos:\")\n",
    "df_clean.orderBy(F.desc(\"VALOR\")).show(10, truncate=False)\n",
    "\n",
    "# Si tiene año, promedio por año\n",
    "if \"AÑO_INICIO\" in df_clean.columns:\n",
    "    print(\"Promedio por año:\")\n",
    "    df_clean.groupBy(\"AÑO_INICIO\") \\\n",
    "        .agg(F.avg(\"VALOR\").alias(\"Promedio_VALOR\")) \\\n",
    "\n",
    "        .orderBy(\"AÑO_INICIO\") \\\n",
    "        .show()\n",
    "\n",
    "output_path = \"hdfs://localhost:9000/Tarea3/processed\"\n",
    "df_clean.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(f\"Resultados procesados guardados en {output_path}\")\n",
    "\n",
    "print(\"Ejecución completa. Spark permanecerá activo 2 minutos para revisión en >\n",
    "time.sleep(120)\n",
    "spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1f65e0-a1c4-41fe-89f9-2f2b39744f7b",
   "metadata": {},
   "source": [
    "## Explicación del desarrollo en Spark\n",
    "Primero se inicio la sesión de Spark que permite ejecutar operaciones distribuidas sobre grandes volúmenes de datos.\n",
    "El nombre de la aplicación (“Tarea3_ProcesamientoBatch”) sirve para identificar el trabajo dentro del entorno de Spark. Además, se ajusta el nivel de log a “WARN” para mostrar solo mensajes importantes.\n",
    "\n",
    "Despues se realizo la carga del dataset desde HDFS , con el archivo rows.csv directamente desde el sistema distribuido HDFS.\n",
    "\n",
    "Posteriormente, se empezó a realizar la limpieza de datos, donde se realizó:\n",
    "- Eliminación de valores nulos: Se borran filas donde el campo “VALOR” (la TRM) no tenga dato.\n",
    "- Conversión de tipo: Se asegura que “VALOR” sea numérico (tipo double), lo cual es necesario para análisis estadístico.\n",
    "- Eliminación de duplicados: Se eliminan filas repetidas para evitar distorsión en los cálculos.\n",
    "\n",
    "En seguida se realizó la creación de una nueva columna derivada, se extrajo el año de la columna VIGENCIADESDE (fecha de inicio de vigencia de la TRM) para permitir análisis temporales por año.\n",
    "\n",
    "Luego se logró realizar un análisis exploratorio (EDA): eL codigo no permitio ver la vista general y esquema por medio del tipo de cada columna y las primeras filas del dataset para entender su estructura,\n",
    "se oncluyeron estadísticas descriptivas, visualización de indicadores agregados: Promedio histórico, valor máximo registrado, valor mínimo registrado; \n",
    "y el promedio de año y el top 10 de valores más altos.\n",
    "\n",
    "Finalmente, el DataFrame limpio y transformado se guarda en HDFS en formato Parquet, que es más eficiente que CSV (mejor compresión y lectura distribuida)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f1b714-72bf-420b-bc61-d0abd34d31d6",
   "metadata": {},
   "source": [
    "## Procesamiento en Tiempo Real (Streaming) \n",
    "Simular la llegada de datos en tiempo real mediante Kafka, procesarlos con Spark Structured Streaming, y calcular métricas agregadas (promedio, máximo, mínimo, conteo) por ventana de tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f9ab9-d62e-4613-84d8-f435bff0573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kafka Producer (Python)\n",
    "#Archivo:\n",
    "kafka_producer.py\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "# === CONFIGURACIÓN ===\n",
    "CSV_PATH = \"/home/vboxuser/rows.csv\"   # Ruta del dataset TRM\n",
    "KAFKA_BOOTSTRAP = \"192.168.80.223:9092\"\n",
    "TOPIC = \"trm_data\"\n",
    "MAX_MESSAGES = 1000\n",
    "SLEEP_SEC = 0.5\n",
    "\n",
    "def create_producer():\n",
    "    return KafkaProducer(\n",
    "        bootstrap_servers=[KAFKA_BOOTSTRAP],\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "        linger_ms=10\n",
    "    )\n",
    "\n",
    "def row_to_message(row, header):\n",
    "    d = {h: row[i] for i, h in enumerate(header)}\n",
    "    if 'VALOR' in d:\n",
    "        try:\n",
    "            d['VALOR'] = float(d['VALOR'])\n",
    "        except:\n",
    "            pass\n",
    "    return d\n",
    "\n",
    "def main():\n",
    "    producer = create_producer()\n",
    "    sent = 0\n",
    "    with open(CSV_PATH, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)\n",
    "        for row in reader:\n",
    "            msg = row_to_message(row, header)\n",
    "            producer.send(TOPIC, value=msg)\n",
    "            sent += 1\n",
    "            if sent % 50 == 0:\n",
    "                producer.flush()\n",
    "            print(f\"Sent {sent}: {msg}\")\n",
    "            if MAX_MESSAGES and sent >= MAX_MESSAGES:\n",
    "                break\n",
    "            time.sleep(SLEEP_SEC)\n",
    "    producer.flush()\n",
    "    producer.close()\n",
    "    print(\"✅ Producer finished. Sent total:\", sent)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a6678a-a6fa-4d85-add5-9a61eea622cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark Streaming Consumer\n",
    "#Archivo:\n",
    "spark_streaming_consumer.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, window, to_timestamp, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "KAFKA_BOOTSTRAP = \"192.168.80.223:9092\"\n",
    "TOPIC = \"trm_data\"\n",
    "CHECKPOINT_LOC = \"/tmp/spark_checkpoint_kafka\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"VALOR\", DoubleType(), True),\n",
    "    StructField(\"UNIDAD\", StringType(), True),\n",
    "    StructField(\"VIGENCIADESDE\", StringType(), True),\n",
    "    StructField(\"VIGENCIAHASTA\", StringType(), True)\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkKafkaConsumer_TRM\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"✅ Spark Streaming Consumer iniciado y conectado a Kafka\")\n",
    "\n",
    "raw = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP) \\\n",
    "    .option(\"subscribe\", TOPIC) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "json_str = raw.selectExpr(\"CAST(value AS STRING) as json_str\")\n",
    "\n",
    "parsed = json_str.select(from_json(col(\"json_str\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "parsed = parsed.filter(col(\"VALOR\").isNotNull())\n",
    "\n",
    "parsed = parsed.withColumn(\n",
    "    \"event_time\",\n",
    "    to_timestamp(col(\"VIGENCIADESDE\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "parsed = parsed.withColumn(\n",
    "    \"event_time\",\n",
    "    expr(\"coalesce(event_time, current_timestamp())\")\n",
    ")\n",
    "\n",
    "agg = parsed.groupBy(\n",
    "    window(col(\"event_time\"), \"1 minute\"),\n",
    "    col(\"UNIDAD\")\n",
    ").agg(\n",
    "    expr(\"count(*) as cnt\"),\n",
    "    expr(\"avg(VALOR) as avg_valor\"),\n",
    "    expr(\"max(VALOR) as max_valor\"),\n",
    "    expr(\"min(VALOR) as min_valor\")\n",
    ").select(\n",
    "    col(\"window.start\").alias(\"window_start\"),\n",
    "    col(\"window.end\").alias(\"window_end\"),\n",
    "    col(\"UNIDAD\"),\n",
    "    col(\"cnt\"),\n",
    "    col(\"avg_valor\"),\n",
    "    col(\"max_valor\"),\n",
    "    col(\"min_valor\")\n",
    ")\n",
    "\n",
    "query = agg.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .option(\"numRows\", 50) \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_LOC) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a1fbf9-ea5d-4be7-8518-4f63a1f49633",
   "metadata": {},
   "source": [
    "## Instrucciones de Ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2066c4ac-aa12-4ad6-bedb-daba068092f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iniciar servicios de Kafka\n",
    "cd /opt/kafka\n",
    "bin/zookeeper-server-start.sh -daemon config/zookeeper.properties\n",
    "bin/kafka-server-start.sh -daemon config/server.properties\n",
    "\n",
    "#Crear el topic\n",
    "bin/kafka-topics.sh --create --topic trm_data --bootstrap-server 192.168.80.223:9092 --partitions 1 --replication-factor 1\n",
    "\n",
    "#Ejecutar el productor\n",
    "python3 kafka_producer.py\n",
    "\n",
    "#Ejecutar el consumidor (Spark)\n",
    "spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3 spark_streaming_consumer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c5da1f-ab76-4674-8b49-24eb6dead2f0",
   "metadata": {},
   "source": [
    "## Resultados esperados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8eba61-1ea6-40a4-91df-3a8dd57bacf1",
   "metadata": {},
   "source": [
    "En consola, Spark mostrará la salida en formato tabular por ventana de tiempo (Batch1, 2, 3, 4....):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e074c0e-0efc-416a-bdde-b28653270bab",
   "metadata": {},
   "source": [
    "## Tecnologías utilizadas\n",
    "\n",
    "Apache Spark 3.5.3\n",
    "\n",
    "Apache Kafka 3.7.2\n",
    "\n",
    "Python 3.10\n",
    "\n",
    "pyspark, kafka-python\n",
    "\n",
    "VirtualBox / Ubuntu 20.04\n",
    "\n",
    "Dataset oficial TRM - Datos Abiertos de Colombia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841666b3-e6a6-4e0c-95dc-5f08fd76a8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
